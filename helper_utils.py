import os
import sys

try:
    # Colab í™˜ê²½ ì—¬ë¶€ í™•ì¸
    import google.colab
    IS_COLAB = True
except ImportError:
    IS_COLAB = False

# --- PyTorch: ë”¥ëŸ¬ë‹ ê´€ë ¨ ---
import torch

# --- ê¸°íƒ€ ---
import requests
import tarfile
import shutil
import json
from datetime import datetime
from pathlib import Path
import re
from tqdm.notebook import tqdm
import numpy as np  # ìˆ˜ì¹˜ ì—°ì‚°
import logging
import pytz
from typing import Union, List
import zipfile
import unicodedata

################################################################################################################

__version__ = "2.6.0"

################################################################################################################

class ShortLevelFormatter(logging.Formatter):
    """ë¡œê·¸ ë ˆë²¨ì„ 1ê¸€ìë¡œ ì¶•ì•½ (DEBUGâ†’D, INFOâ†’I, WARNINGâ†’W, ERRORâ†’E, CRITICALâ†’C)"""

    LEVEL_MAP = {
        'DEBUG': 'D',
        'INFO': 'I',
        'WARNING': 'W',
        'ERROR': 'E',
        'CRITICAL': 'C'
    }
    kst = pytz.timezone('Asia/Seoul')

    def format(self, record):
        # ì›ë³¸ ë ˆë²¨ëª…ì„ ì•½ìë¡œ êµì²´
        record.levelname = self.LEVEL_MAP.get(record.levelname, record.levelname)
        return super().format(record)

    def formatTime(self, record, datefmt=None):
        """record.createdë¥¼ KSTë¡œ ë³€í™˜í•´ í¬ë§·ëœ ë¬¸ìì—´ ë°˜í™˜"""
        ct = datetime.fromtimestamp(record.created, tz=self.kst)
        if datefmt:
            return ct.strftime(datefmt)
        return ct.strftime('%Y-%m-%d %H:%M:%S')
    
if IS_COLAB:
    # Colab: ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° í›„ ì¬ì„¤ì •
    logger = logging.getLogger()
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    handler = logging.StreamHandler()
    handler.setFormatter(ShortLevelFormatter(
        '%(asctime)s [%(levelname)s] %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    ))
    logger.addHandler(handler)
else:
    logging.basicConfig(
        format='%(asctime)s [%(levelname)s] %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    logger = logging.getLogger()
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ì˜ Formatter êµì²´
    for handler in logging.getLogger().handlers:
        handler.setFormatter(ShortLevelFormatter(
            '%(asctime)s [%(levelname)s] %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        ))

# logger.setLevel(logging.DEBUG)

################################################################################################################

class AIHubShell:
    def __init__(self, DEBUG=False, download_dir=None):
        self.BASE_URL = "https://api.aihub.or.kr"
        self.LOGIN_URL = f"{self.BASE_URL}/api/keyValidate.do"
        self.BASE_DOWNLOAD_URL = f"{self.BASE_URL}/down/0.5"
        self.MANUAL_URL = f"{self.BASE_URL}/info/api.do"
        self.BASE_FILETREE_URL = f"{self.BASE_URL}/info"
        self.DATASET_URL = f"{self.BASE_URL}/info/dataset.do"
        self.DEBUG = DEBUG
        self.download_dir = download_dir if download_dir else "."
                
    def help(self):
        """AIHubShell í´ë˜ìŠ¤ ì‚¬ìš©ë²• ì¶œë ¥"""
        print("=" * 80)
        print("                        AIHubShell í´ë˜ìŠ¤ ì‚¬ìš© ê°€ì´ë“œ")
        print("=" * 80)
        print()
        
        print("ğŸ”§ ì´ˆê¸°í™”")
        print("  AIHubShell(DEBUG=False, download_dir=None)")
        print("    DEBUG: Trueë¡œ ì„¤ì •í•˜ë©´ ìƒì„¸ ë¡œê·¸ ì¶œë ¥")
        print("    download_dir: ë‹¤ìš´ë¡œë“œ ê²½ë¡œ ì§€ì • (ê¸°ë³¸ê°’: í˜„ì¬ ê²½ë¡œ)")
        print()
        
        print("ğŸ“‹ ë°ì´í„°ì…‹ ì¡°íšŒ")
        print("  .dataset_info()                    # ì „ì²´ ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ")
        print("  .dataset_search('ê²€ìƒ‰ì–´')          # íŠ¹ì • ì´ë¦„ í¬í•¨ ë°ì´í„°ì…‹ ê²€ìƒ‰")
        print("  .dataset_search('ê²€ìƒ‰ì–´', tree=True) # ê²€ìƒ‰ + íŒŒì¼ íŠ¸ë¦¬ ì¡°íšŒ")
        print("  .list_info(datasetkey=576)         # íŠ¹ì • ë°ì´í„°ì…‹ì˜ íŒŒì¼ ëª©ë¡")
        print("  .json_info(datasetkey=576)         # JSON í˜•íƒœë¡œ íŒŒì¼ êµ¬ì¡° ë°˜í™˜")
        print()
        
        print("ğŸ’¾ ë‹¤ìš´ë¡œë“œ")
        print("  .download_dataset(apikey, datasetkey, filekeys='all')")
        print("    apikey: AI Hub API í‚¤")
        print("    datasetkey: ë°ì´í„°ì…‹ ë²ˆí˜¸")
        print("    filekeys: íŒŒì¼í‚¤ ('all' ë˜ëŠ” '66065,66083' í˜•íƒœ)")
        print("    overwrite: ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸° ì—¬ë¶€ (ê¸°ë³¸ê°’: False)")
        print()
        
        print("ğŸ“– ê¸°íƒ€ ê¸°ëŠ¥")
        print("  .print_usage()                     # AI Hub API ìƒì„¸ ì‚¬ìš©ë²•")
        print("  .help()                            # ì´ ë„ì›€ë§")
        print()
        
        print("ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ")
        print("  # 1. ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
        print("  aihub = AIHubShell(DEBUG=True, download_dir='./data')")
        print()
        print("  # 2. ê²½êµ¬ì•½ì œ ë°ì´í„°ì…‹ ê²€ìƒ‰")
        print("  aihub.dataset_search('ê²½êµ¬ì•½ì œ')")
        print()
        print("  # 3. ë°ì´í„°ì…‹ 576ì˜ íŒŒì¼ ëª©ë¡ í™•ì¸")
        print("  aihub.list_info(datasetkey=576)")
        print()
        print("  # 4. íŠ¹ì • íŒŒì¼ë“¤ë§Œ ë‹¤ìš´ë¡œë“œ")
        print("  aihub.download_dataset(")
        print("      apikey='YOUR_API_KEY',")
        print("      datasetkey=576,")
        print("      filekeys='66065,66083'")
        print("  )")
        print()
        print("  # 5. ì „ì²´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ")
        print("  aihub.download_dataset(")
        print("      apikey='YOUR_API_KEY',")
        print("      datasetkey=576,")
        print("      filekeys='all'")
        print("  )")
        print()
        
        print("âš ï¸  ì£¼ì˜ì‚¬í•­")
        print("  - API í‚¤ëŠ” AI Hubì—ì„œ ë°œê¸‰ë°›ì•„ì•¼ í•©ë‹ˆë‹¤")
        print("  - ëŒ€ìš©ëŸ‰ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œ ì¶©ë¶„í•œ ì €ì¥ ê³µê°„ì„ í™•ë³´í•˜ì„¸ìš”")
        print("  - overwrite=Falseì¼ ë•Œ ê¸°ì¡´ íŒŒì¼ì€ ìë™ìœ¼ë¡œ ê±´ë„ˆëœë‹ˆë‹¤")
        print("  - ë„¤íŠ¸ì›Œí¬ ìƒíƒœì— ë”°ë¼ ë‹¤ìš´ë¡œë“œ ì‹œê°„ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        print()
        
        print("ğŸ” ì¶”ê°€ ì •ë³´")
        print("  AI Hub API ê³µì‹ ë¬¸ì„œ: https://aihub.or.kr")
        print("  ë¬¸ì œ ë°œìƒ ì‹œ DEBUG=Trueë¡œ ì„¤ì •í•˜ì—¬ ìƒì„¸ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”")
        print("=" * 80)
                        
    def print_usage(self):
        """ì‚¬ìš©ë²• ì¶œë ¥"""
        try:
            response = requests.get(self.MANUAL_URL)
            manual = response.text
            
            if self.DEBUG:
                print("API ì›ë³¸ ì‘ë‹µ:")
                print(manual)            
            
            # JSON íŒŒì‹±í•˜ì—¬ ë°ì´í„° ì¶”ì¶œ
            try:
                manual = re.sub(r'("FRST_RGST_PNTTM":)([0-9\- :\.]+)', r'\1"\2"', manual)
                manual_data = json.loads(manual)
                if self.DEBUG:
                    print("JSON íŒŒì‹± ì„±ê³µ")
                    
                if 'result' in manual_data and len(manual_data['result']) > 0:
                    print(manual_data['result'][0].get('SJ', ''))
                    print()
                    print("ENGL_CMGG\t KOREAN_CMGG\t\t\t DETAIL_CN")
                    print("-" * 80)
                    
                    for item in manual_data['result']:
                        engl = item.get('ENGL_CMGG', '')
                        korean = item.get('KOREAN_CMGG', '')
                        detail = item.get('DETAIL_CN', '').replace('\\n', '\n').replace('\\t', '\t')
                        print(f"{engl:<10}\t {korean:<15}\t|\t {detail}\n")
            except json.JSONDecodeError:
                if self.DEBUG:
                    print("JSON íŒŒì‹± ì˜¤ë¥˜:", e)
                else:
                    print("API ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜")
        except requests.RequestException as e:
            print(f"API ìš”ì²­ ì˜¤ë¥˜: {e}")
    
    def _merge_parts(self, target_dir):
        """part íŒŒì¼ë“¤ì„ ë³‘í•©"""
        target_path = Path(target_dir)
        part_files = list(target_path.glob("*.part*"))
        
        if not part_files:
            return
            
        # prefixë³„ë¡œ ê·¸ë£¹í™”
        prefixes = {}
        for part_file in part_files:
            match = re.match(r'(.+)\.part(\d+)$', part_file.name)
            if match:
                prefix = match.group(1)
                part_num = int(match.group(2))
                if prefix not in prefixes:
                    prefixes[prefix] = []
                prefixes[prefix].append((part_num, part_file))
        
        # ê° prefixë³„ë¡œ ë³‘í•©
        for prefix, parts in prefixes.items():
            print(f"Merging {prefix} in {target_dir}")
            parts.sort(key=lambda x: x[0])  # part ë²ˆí˜¸ë¡œ ì •ë ¬
            
            output_path = target_path / prefix
            with open(output_path, 'wb') as output_file:
                for _, part_file in parts:
                    with open(part_file, 'rb') as input_file:
                        shutil.copyfileobj(input_file, output_file)
            
            # part íŒŒì¼ë“¤ ì‚­ì œ
            for _, part_file in parts:
                part_file.unlink()
                
    def _merge_parts_all(self, base_path="."):
        """ëª¨ë“  í•˜ìœ„ í´ë”ì˜ part íŒŒì¼ë“¤ì„ ë³‘í•©"""
        if self.DEBUG:
            print("ë³‘í•© ì¤‘ì…ë‹ˆë‹¤...")
        for root, dirs, files in os.walk(base_path):
            part_files = [f for f in files if '.part' in f]
            if part_files:
                self._merge_parts(root)
        if self.DEBUG:
            print("ë³‘í•©ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def download_dataset(self, apikey, datasetkey, filekeys="all", overwrite=False):
        """ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (ì˜µì…˜: ë®ì–´ì“°ê¸°)"""
        def _parse_size(size_str):
            """'92 GB', '8 MB' ë“± ë¬¸ìì—´ì„ ë°”ì´íŠ¸ ë‹¨ìœ„ë¡œ ë³€í™˜"""
            size_str = size_str.strip().upper()
            if 'GB' in size_str:
                return float(size_str.replace('GB', '').strip()) * 1024**3
            elif 'MB' in size_str:
                return float(size_str.replace('MB', '').strip()) * 1024**2
            elif 'KB' in size_str:
                return float(size_str.replace('KB', '').strip()) * 1024
            elif 'B' in size_str:
                return float(size_str.replace('B', '').strip())
            return 0
        
        download_path = Path(self.download_dir)
        download_tar_path = download_path / "download.tar"
        
        download_list = self.list_info(datasetkey=datasetkey, filekeys=filekeys, print_out=False)
        
        # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ì€ ì œì™¸
        keys_to_download = []
        for key, info in download_list.items():
            extracted_file_path = os.path.join(self.download_dir, info.path)
            if not overwrite and os.path.exists(extracted_file_path):
                print(f"íŒŒì¼ ë°œê²¬: {extracted_file_path}")
                if self.DEBUG:
                    print("ë‹¤ìš´ë¡œë“œë¥¼ ìƒëµí•©ë‹ˆë‹¤.")
                continue
            
            # ì••ì¶• í•´ì§€ í•˜ê³  ìš©ëŸ‰ ì´ìŠˆë¡œ ì¸í•˜ì—¬ zipíŒŒì¼ì€ ì‚­ì œ ë˜ì—ˆë‹¤.
            if not overwrite and os.path.exists(extracted_file_path + ".unzip"):
                print(f"íŒŒì¼ ë°œê²¬ unzip: {extracted_file_path}.unzip")
                if self.DEBUG:
                    print("ë‹¤ìš´ë¡œë“œë¥¼ ìƒëµí•©ë‹ˆë‹¤.")
                continue
            
            keys_to_download.append(str(key))

        # ë‹¤ìš´ë¡œë“œí•  filekeysê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
        if not keys_to_download:
            print("ëª¨ë“  íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
            extracted_files = []
            for key, info in download_list.items():
                file_path = os.path.join(self.download_dir, info.path)
                if os.path.exists(file_path):
                    extracted_files.append(file_path)
            print("ë‹¤ìš´ë¡œë“œ íŒŒì¼ ëª©ë¡:", extracted_files)
            return extracted_files            

        # í—¤ë”ì™€ íŒŒë¼ë¯¸í„° ê¸°ë³¸ ì„¤ì •
        headers = {"apikey": apikey}
        params = {"fileSn": ",".join(keys_to_download)}
        
        mode = "wb"
        existing_size = 0
        response_head = requests.head(f"{self.BASE_DOWNLOAD_URL}/{datasetkey}.do", headers=headers, params=params)
        if "content-length" in response_head.headers:
            total_size = int(response_head.headers.get('content-length', 0))
        else:
            total_size = 0
            if self.DEBUG:
                print("content-length í—¤ë”ê°€ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ í¬ê¸° ì•Œ ìˆ˜ ì—†ìŒ.")
                print("HEAD ì‘ë‹µ í—¤ë”:", response_head.headers)

        if total_size == 0:
            total_size = int(sum(_parse_size(info.size) for info in download_list.values()))
            if self.DEBUG:
                print(f"download_list ê¸°ë°˜ ì¶”ì • total_size: {total_size / (1024**3):.2f} GB")
                
        # ì‹¤ì œ ë‹¤ìš´ë¡œë“œ
        if self.DEBUG:
            print("ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
            
        os.makedirs(download_path, exist_ok=True)
        response = requests.get(
            f"{self.BASE_DOWNLOAD_URL}/{datasetkey}.do",
            headers=headers,
            params=params,
            stream=True
        )

        if response.status_code in [200, 206]:
            
            with open(download_tar_path, mode) as f, tqdm(
                total=total_size, 
                unit='B', 
                unit_scale=True, 
                desc="Downloading", 
                mininterval=3.0,  # 3ì´ˆë§ˆë‹¤ ê°±ì‹ 
                initial=(existing_size if mode == "ab" else 0)
            ) as pbar:
                update_count = 1000
                downloaded = existing_size if mode == "ab" else 0
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
                    #f.flush()
                    downloaded += len(chunk)
                    pbar.update(len(chunk))
                    if update_count <= 0:
                        pbar.set_postfix_str(f"{downloaded / (1024**2):.2f}MB / {total_size / (1024**2):.2f}MB")
                        update_count = 1000
                    update_count -= 1
                f.flush()
            
            if self.DEBUG:
                print("ì••ì¶• í•´ì œ ì¤‘...")
            with tarfile.open(download_tar_path, "r") as tar:
                tar.extractall(path=download_path)
            self._merge_parts_all(download_path)
            download_tar_path.unlink()
            
            print("ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        else:
            print(f"Download failed with HTTP status {response.status_code}.")
            print("Error msg:")
            print(response.text)
            if download_tar_path.exists():
                download_tar_path.unlink()
                
        extracted_files = []
        for key, info in download_list.items():
            file_path = os.path.join(self.download_dir, info.path)
            if os.path.exists(file_path):
                extracted_files.append(file_path)
        print("ë‹¤ìš´ë¡œë“œ íŒŒì¼ ëª©ë¡:", extracted_files)
        return extracted_files            
                
    def list_info(self, datasetkey=None, filekeys="all", print_out=True):
        """ë°ì´í„°ì…‹ íŒŒì¼ ì •ë³´ ì¡°íšŒ (filekeys, íŒŒì¼ëª…, ì‚¬ì´ì¦ˆ ì¶œë ¥ ë° ë”•ì…”ë„ˆë¦¬ ë°˜í™˜)"""
        resjson = self.json_info(datasetkey=datasetkey)
        
        # íŒŒì¼ ì •ë³´ë¥¼ ë‹´ì„ ë”•ì…”ë„ˆë¦¬
        file_info_dict = {}
        
        def extract_files(structure):
            """ì¬ê·€ì ìœ¼ë¡œ íŒŒì¼ ì •ë³´ ì¶”ì¶œ"""
            for item in structure:
                if item["type"] == "file" and "filekey" in item:
                    filekey = int(item["filekey"])
                    file_info_dict[filekey] = {
                        "filekey": item["filekey"],
                        "filename": item["name"],
                        "size": item["size"],
                        "path": item["path"],
                        "deep": item["deep"]
                    }
                elif item["type"] == "directory" and "children" in item:
                    extract_files(item["children"])
        
        # JSON êµ¬ì¡°ì—ì„œ íŒŒì¼ ì •ë³´ ì¶”ì¶œ
        extract_files(resjson["structure"])
        
        # filekeys ì²˜ë¦¬
        if filekeys == "all":
            filtered_files = file_info_dict
        else:
            # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ filekeys íŒŒì‹±
            requested_keys = []
            for key in filekeys.split(','):
                try:
                    requested_keys.append(int(key.strip()))
                except ValueError:
                    continue
            
            # ìš”ì²­ëœ filekeyë§Œ í•„í„°ë§
            filtered_files = {k: v for k, v in file_info_dict.items() if k in requested_keys}
        
        # ì¶œë ¥
        if print_out:
            print(f"Dataset: {datasetkey}")
            print("=" * 80)
            print(f"{'FileKey':<8} {'Filename':<30} {'Size':<10} {'Path'}")
            print("-" * 80)
            
            for filekey, info in sorted(filtered_files.items()):
                print(f"{info['filekey']:<8} {info['filename']:<30} {info['size']:<10} {info['path']}")
            
            print(f"\nì´ {len(filtered_files)}ê°œ íŒŒì¼")
        
        # ë”•ì…”ë„ˆë¦¬ ë°˜í™˜ (FileInfo ê°ì²´ í˜•íƒœë¡œ)
        class FileInfo:
            def __init__(self, filekey, filename, size, path, deep):
                self.filekey = filekey
                self.filename = filename
                self.size = size
                self.path = path
                self.deep = deep
            
            def __str__(self):
                return f"FileInfo(filekey={self.filekey}, filename='{self.filename}', size='{self.size}' , path='{self.path}', deep={self.deep})"
            
            def __repr__(self):
                return self.__str__()
        
        result_dict = {}
        for filekey, info in filtered_files.items():
            result_dict[filekey] = FileInfo(
                filekey=info["filekey"],
                filename=info["filename"],
                size=info["size"],
                path=info["path"],
                deep=info["deep"]
            )
        
        return result_dict
        
    # filepath: [ê²½êµ¬ì•½ì œ_ì´ë¯¸ì§€_ë°ì´í„°.ipynb](http://_vscodecontentref_/0)
    def dataset_info(self, datasetkey=None, datasetname=None):
        """ë°ì´í„°ì…‹ ëª©ë¡ ë˜ëŠ” íŒŒì¼ íŠ¸ë¦¬ ì¡°íšŒ"""
        if datasetkey:
            filetree_url = f"{self.BASE_FILETREE_URL}/{datasetkey}.do"
            print("Fetching file tree structure...")
            try:
                response = requests.get(filetree_url)
                # ì¸ì½”ë”© ìë™ ê°ì§€
                response.encoding = response.apparent_encoding
                print(response.text)
            except requests.RequestException as e:
                print(f"API ìš”ì²­ ì˜¤ë¥˜: {e}")
        else:
            print("Fetching dataset information...")
            try:
                response = requests.get(self.DATASET_URL)
                response.encoding = 'utf-8'
                #response.encoding = 'euc-kr'
                print(response.text)
            except requests.RequestException as e:
                print(f"API ìš”ì²­ ì˜¤ë¥˜: {e}")

    def dataset_search(self, datasetname=None, tree=False):
        """
        ë°ì´í„°ì…‹ ëª©ë¡ ë˜ëŠ” íŠ¹ì • ì´ë¦„ì´ í¬í•¨ëœ ë°ì´í„°ì…‹ì˜ íŒŒì¼ íŠ¸ë¦¬ ì¡°íšŒ
        datasetname: ê²€ìƒ‰í•  ë°ì´í„°ì…‹ ì´ë¦„ (ë¶€ë¶„ ì¼ì¹˜)
        tree: Trueì´ë©´ í•´ë‹¹ ë°ì´í„°ì…‹ì˜ íŒŒì¼ íŠ¸ë¦¬ë„ ì¡°íšŒ        
        """
        print("Fetching dataset information...")
        try:
            response = requests.get(self.DATASET_URL)
            response.encoding = 'utf-8'
            text = response.text
            if datasetname:
                # datasetnameì´ í¬í•¨ëœ ë¶€ë¶„ë§Œ ì¶œë ¥
                lines = text.splitlines()
                for line in lines:
                    if datasetname in line:
                        #print(line)
                        # 576, ê²½êµ¬ì•½ì œ ì´ë¯¸ì§€ ë°ì´í„°
                        num, name = line.split(',', 1)
                        # í•´ë‹¹ ë°ì´í„°ì…‹ì˜ íŒŒì¼ íŠ¸ë¦¬ ì¡°íšŒ
                        if tree:
                            self.dataset_info(datasetkey=num.strip())
                        else:
                            print(line)
            else:
                print(text)
        except requests.RequestException as e:
            print(f"API ìš”ì²­ ì˜¤ë¥˜: {e}")

    def _get_depth_from_star_count(self, star_count, depth_mapping):
        """star_count ê°’ì„ ê¹Šì´(deep)ë¡œ ë³€í™˜"""
        if star_count not in depth_mapping:
            # ìƒˆë¡œìš´ star_count ê°’ì´ë©´ ë°°ì—´ì— ì¶”ê°€
            depth_mapping.append(star_count)
            # ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬
            depth_mapping.sort()
        
        # ë°°ì—´ì—ì„œì˜ ì¸ë±ìŠ¤ê°€ ê¹Šì´
        return depth_mapping.index(star_count)

    def _json_line(self, line, json_obj, depth_mapping, path_stack, weight=0, deep=0):
        """íŒŒì¼ íŠ¸ë¦¬ì˜ í•œ ì¤„ì„ JSON êµ¬ì¡°ì— ë§ê²Œ íŒŒì‹±í•˜ì—¬ ì¶”ê°€"""
        # íŠ¸ë¦¬ êµ¬ì¡° ê¸°í˜¸ë¥¼ ëª¨ë‘ *ë¡œ ë³€ê²½
        line = line.replace("â”œâ”€", "â””â”€")
        line = line.replace("â”‚ ", "â””â”€")
        while "    â””â”€" in line:
            line = line.replace("    â””â”€", "â””â”€â””â”€")
        while " â””â”€" in line:
            line = line.replace(" â””â”€", "â””â”€")
        
        while "â””â”€" in line:
            line = line.replace("â””â”€", "*")
        
        # ì•ë¶€ë¶„ì˜ * ê°œìˆ˜ì™€ ë¬¸ìì—´ ì¶”ì¶œ
        star_count = 0
        for char in line:
            if char == '*':
                star_count += 1
            else:
                break
        clean_str = line.replace('*', '').strip()
        
        # star_countë¥¼ deepë¡œ ë™ì  ë³€í™˜
        deep = self._get_depth_from_star_count(star_count, depth_mapping)
        
        has_pipe = "|" in line
        
        # íŒŒì¼/í´ë” ì •ë³´ ì¶”ì¶œ
        if has_pipe:
            parts = clean_str.split('|')
            if len(parts) >= 3:
                filename = parts[0].strip()
                size = parts[1].strip()
                filekey = parts[2].strip()
                item_type = "file"
            else:
                filename = clean_str
                size = ""
                filekey = ""
                item_type = "directory"
        else:
            filename = clean_str
            size = ""
            filekey = ""
            item_type = "directory"
        
        # path_stack ì¡°ì • (í˜„ì¬ ê¹Šì´ì— ë§ê²Œ)
        while len(path_stack) > deep:
            path_stack.pop()
        
        # í˜„ì¬ ì•„ì´í…œ ì •ë³´
        current_item = {
            "name": filename,
            "type": item_type,
            "deep": deep,
            "weight": star_count,
            "path": str(Path(*path_stack, filename)).replace(' ', '_')  # ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€ê²½
        }
        
        if item_type == "file":
            current_item["size"] = size
            current_item["filekey"] = filekey
        else:
            current_item["children"] = []
        
        # JSON êµ¬ì¡°ì— ì¶”ê°€ (ë°°ì—´ êµ¬ì¡°)
        current_array = json_obj
        for path_name in path_stack:
            # í•´ë‹¹ ì´ë¦„ì˜ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì•„ì„œ ê·¸ children ë°°ì—´ë¡œ ì´ë™
            found = None
            for item in current_array:
                if item["name"] == path_name and item["type"] == "directory":
                    found = item
                    break
            if found:
                current_array = found["children"]
        
        # í˜„ì¬ ë°°ì—´ì— ì•„ì´í…œ ì¶”ê°€
        current_array.append(current_item)
        
        # ë””ë ‰í† ë¦¬ì¸ ê²½ìš° path_stackì— ì¶”ê°€
        if item_type == "directory":
            path_stack.append(filename)
        
        # if self.DEBUG:
        #     print(f"[deep={deep}] [weight={star_count}] {item_type[0].upper()} {filename}" + 
        #         (f" , {size} , {filekey}" if item_type == "file" else " , , "))
        
        return current_item

    def json_info(self, datasetkey=None):
        """ë°ì´í„°ì…‹ ëª©ë¡ ë˜ëŠ” íŒŒì¼ íŠ¸ë¦¬ë¥¼ JSON í˜•íƒœë¡œ ë°˜í™˜"""
        filetree_url = f"{self.BASE_FILETREE_URL}/{datasetkey}.do"        
        response = requests.get(filetree_url)
        response.encoding = response.apparent_encoding
        text = response.text
        
        # JSON êµ¬ì¡°ë¥¼ ìœ„í•œ ë”•ì…”ë„ˆë¦¬
        result = {
            "datasetkey": datasetkey,
            "structure": []  # ë°°ì—´ë¡œ ë³€ê²½
        }
        
        lines = text.splitlines()
        
        is_notify = True
        json_obj = []  # ë£¨íŠ¸ ë°°ì—´
        depth_mapping = []  # ê° íŒŒì‹± ì„¸ì…˜ë§ˆë‹¤ ìƒˆë¡œìš´ depth_mapping
        path_stack = []     # í˜„ì¬ ê²½ë¡œë¥¼ ì¶”ì í•˜ëŠ” ìŠ¤íƒ

        # if self.DEBUG:
        #     test_count = 10

        for line in lines:
            if not line.strip() or 'ê³µì§€ì‚¬í•­' in line or '=' in line:
                is_notify = False
                continue
            if is_notify:
                continue

            self._json_line(line, json_obj, depth_mapping, path_stack, weight=0, deep=0)

            # if self.DEBUG:
            #     test_count -= 1
            #     if test_count <= 0:
            #         break
        
        result["structure"] = json_obj
        
        return result
################################################################################################################

def get_tqdm_kwargs():
    """Widget ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ëŠ” ì•ˆì „í•œ tqdm ì„¤ì •"""
    return {
        'disable': False,
        'leave': True,
        'file': sys.stdout,
        'ascii': True,  # ASCII ë¬¸ìë§Œ ì‚¬ìš©
        'dynamic_ncols': False,
#        'ncols': 80  # ê³ ì • í­
    }

def drive_root():
    root_path = os.path.join("D:\\", "GoogleDrive")
    if IS_COLAB:
        root_path = os.path.join("/content/drive/MyDrive")
    return root_path

def get_path_modeling(add_path = None):
    modeling_path = "modeling"
    path = os.path.join(drive_root(),modeling_path)
    if add_path is not None:
        path = os.path.join(path,add_path)
    return path

def get_path_modeling_release(add_path = None):
    modeling_path = "modeling_release"
    path = os.path.join(drive_root(),modeling_path)
    if add_path is not None:
        path = os.path.join(path,add_path)
    return path
    
def get_path_temp(add_path = None):
    """ì„ì‹œ ê²½ë¡œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

    Args:
        add_path (str, optional): ì¶”ê°€ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤. Defaults to None.

    Returns:
        str: ì„ì‹œ ê²½ë¡œ ë¬¸ìì—´
    """
    if IS_COLAB:
        temp_path = r"/content/temp"
    else:
        drive = os.path.splitdrive(os.getcwd())[0]  # ex: 'D:'
        temp_path = os.path.join(drive + os.sep, 'temp')
    if add_path is not None:
        temp_path = os.path.join(temp_path,add_path)
    return temp_path

################################################################################################################
def download_gdrive_file(url : str, output_path: str, ignore=True):
    try:
        import gdown
    except ImportError:
        raise ImportError("gdown ëª¨ë“ˆì´ í•„ìš”í•©ë‹ˆë‹¤. 'pip install gdown'ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")

    """Google Drive íŒŒì¼ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜

    Args:
        url (str): Google Drive ê³µìœ  ë§í¬
        output_path (str): ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ
        ignore (bool, optional): Trueë©´ ê¸°ì¡´ íŒŒì¼ ì‚­ì œ í›„ ë‹¤ìš´ë¡œë“œ, Falseë©´ íŒŒì¼ ìˆìœ¼ë©´ ê±´ë„ˆëœ€. Defaults to True.

    Raises:
        ValueError: Google Drive íŒŒì¼ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
    """
    # ê³µìœ  ë§í¬ì—ì„œ íŒŒì¼ ID ì¶”ì¶œ
    if os.path.exists(output_path):
        if ignore:
            os.remove(output_path)
        else:
            return

    file_id_match = re.search(r'/d/([a-zA-Z0-9_-]+)', url)
    if not file_id_match:
        raise ValueError("Google Drive íŒŒì¼ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    file_id = file_id_match.group(1)

    gdown.download(f"https://drive.google.com/uc?id={file_id}", output_path, quiet=False)

def download_http(url: str, output_path: str, ignore=True):
    """
    HTTP íŒŒì¼ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ (ì§„í–‰ë¥  í‘œì‹œ)
    url: ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ URL
    output_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
    ignore: Trueë©´ ê¸°ì¡´ íŒŒì¼ ì‚­ì œ í›„ ë‹¤ìš´ë¡œë“œ, Falseë©´ íŒŒì¼ ìˆìœ¼ë©´ ê±´ë„ˆëœ€
    """
    if os.path.exists(output_path):
        if ignore:
            os.remove(output_path)
        else:
            print(f"ì´ë¯¸ íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤: {output_path}")
            return output_path

    response = requests.get(url, stream=True)
    total = int(response.headers.get('content-length', 0))
    with open(output_path, 'wb') as file, tqdm(
        desc=f"Downloading {os.path.basename(output_path)}",
        total=total,
        unit='B',
        unit_scale=True,
        unit_divisor=1024,
        ascii=True
    ) as bar:
        for data in response.iter_content(chunk_size=1024):
            size = file.write(data)
            bar.update(size)
    print(f"ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {output_path}")
    return output_path

################################################################################################################

def print_dir_tree(root, indent=""):
    """ë””ë ‰í† ë¦¬ íŠ¸ë¦¬ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.

    Args:
        root (str): ì‹œì‘ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        max_depth (int, optional): ìµœëŒ€ ê¹Šì´. Defaults to 2.
        indent (str, optional): ë“¤ì—¬ì“°ê¸° ë¬¸ìì—´. Defaults to "".
    """
    import os
    try:
        items = os.listdir(root)
    except Exception as e:
        print(indent + f"[Error] {e}")
        return

    img_count = len([f for f in os.listdir(root)])
    for item in items:
        path = os.path.join(root, item)
        if os.path.isdir(path):
            print(indent + "|-- "+ item)
            # ì´ë¯¸ì§€ íŒŒì¼ ê°œìˆ˜ë§Œ ì¶œë ¥
            img_count = len([f for f in os.listdir(path)])
            print(indent + "   "+ f"[ë°ì´í„°íŒŒì¼: {img_count}ê°œ]")
            print_dir_tree(root=path, indent=indent + "   ")
        else:
            print(indent + "|-- "+ item)
            

def print_json_tree(data, indent="", max_depth=4, _depth=0, list_count=2, print_value=True, limit_value_text=100):
    """
    JSON ê°ì²´ë¥¼ ì§€ì •í•œ ë‹¨ê³„(max_depth)ê¹Œì§€ íŠ¸ë¦¬ í˜•íƒœë¡œ ì¶œë ¥
    - list íƒ€ì…ì€ 3ê°œ ì´ìƒì¼ ë•Œ ê°œìˆ˜ë§Œ ì¶œë ¥
    - í•˜ìœ„ ë…¸ë“œê°€ ê°’ì¼ ê²½ìš° key(type) í˜•íƒœë¡œ ì¶œë ¥
    - print_value=Trueì¼ ë•Œ key(type): ê°’ í˜•íƒœë¡œ ì¶œë ¥
    """
    if _depth > max_depth:
        return
    if isinstance(data, dict):
        for key, value in data.items():
            if isinstance(value, (dict, list)):
                print(f"{indent}|-- {key}")
                print_json_tree(value, indent + "    ", max_depth, _depth + 1, list_count, print_value)
            else:
                if print_value:
                    print(f"{indent}|-- {key}({type(value).__name__}): {value if len(str(value)) < limit_value_text else f'{str(value)[:30]}...'}")
                else:
                    print(f"{indent}|-- {key}({type(value).__name__})")
    elif isinstance(data, list):
        if len(data) > list_count:
            print(f"{indent}|-- [list] ({len(data)} items)")
        else:
            for i, item in enumerate(data):
                if isinstance(item, (dict, list)):
                    print(f"{indent}|-- [{i}]")
                    print_json_tree(item, indent + "    ", max_depth, _depth + 1, list_count, print_value)
                else:
                    if print_value:
                        print(f"{indent}|-- [{i}]({type(item).__name__}): {item if len(str(item)) < limit_value_text else f'{str(item)[:30]}...'}")
                    else:
                        print(f"{indent}|-- [{i}]({type(item).__name__})")
    else:
        if print_value:
            print(f"{indent}{type(data).__name__}: {data if len(str(data)) < limit_value_text else f'{str(data)[:30]}...'}")
        else:
            print(f"{indent}{type(data).__name__}")

def print_dic_tree(dic_data, indent="", max_depth=3, _depth=0):
    """
    PyTorch tensor/ë”•ì…”ë„ˆë¦¬/ë¦¬ìŠ¤íŠ¸ë¥¼ git tree ìŠ¤íƒ€ì¼ë¡œ ì¶œë ¥
    """
    import torch
    import numpy as np

    if _depth > max_depth:
        return
    if isinstance(dic_data, dict):
        for key, value in dic_data.items():
            print(f"{indent}â”œâ”€ {key} [{type(value).__name__}]")
            print_dic_tree(value, indent + "â”‚  ", max_depth, _depth + 1)
    elif isinstance(dic_data, (list, tuple)):
        for i, item in enumerate(dic_data):
            print(f"{indent}â”œâ”€ [{i}] [{type(item).__name__}]")
            print_dic_tree(item, indent + "â”‚  ", max_depth, _depth + 1)
    elif torch.is_tensor(dic_data):
        shape = tuple(dic_data.shape)
        dtype = str(dic_data.dtype)
        preview = str(dic_data)
        preview_str = preview[:80] + ("..." if len(preview) > 80 else "")
        print(f"{indent}â””â”€ Tensor shape={shape} dtype={dtype} preview={preview_str}")
    elif isinstance(dic_data, np.ndarray):
        shape = dic_data.shape
        dtype = dic_data.dtype
        preview = str(dic_data)
        preview_str = preview[:80] + ("..." if len(preview) > 80 else "")
        print(f"{indent}â””â”€ ndarray shape={shape} dtype={dtype} preview={preview_str}")
    else:
        val_str = str(dic_data)
        print(f"{indent}â””â”€ {type(dic_data).__name__}: {val_str[:80]}{'...' if len(val_str)>80 else ''}")

################################################################################################################

def save_model_dict(model, path, pth_name, kwargs=None):
    """
    ëª¨ë¸ state_dictì™€ ì¶”ê°€ ì •ë³´ë¥¼ ì €ì¥
    """
    def safe_makedirs(path):
        """ì•ˆì „í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        if os.path.exists(path) and not os.path.isdir(path):
            os.remove(path)  # íŒŒì¼ì´ë©´ ì‚­ì œ
        os.makedirs(path, exist_ok=True)

    # ë””ë ‰í† ë¦¬ ìƒì„±
    safe_makedirs(path)

    # ëª¨ë¸ êµ¬ì¡° ì •ë³´ ì¶”ì¶œ
    model_info = {
        'class_name': model.__class__.__name__,
        'init_args': {},
        'str': str(model),
        'repr': repr(model),
        'modules': [m.__class__.__name__ for m in model.modules()],
    }

    # ìƒì„±ì ì¸ì ìë™ ì¶”ì¶œ(ê°€ëŠ¥í•œ ê²½ìš°)
    if hasattr(model, '__dict__'):
        for key in ['in_ch', 'base_ch', 'num_classes', 'out_ch']:
            if hasattr(model, key):
                model_info['init_args'][key] = getattr(model, key)

    # kwargs ì²˜ë¦¬
    extra_info = {}
    if kwargs is not None:
        if isinstance(kwargs, str):
            extra_info = json.loads(kwargs)
        elif isinstance(kwargs, dict):
            extra_info = kwargs

    model_info.update(extra_info)

    # ì €ì¥í•  dict êµ¬ì„±
    save_dict = {
        'model_state': model.state_dict(),
        'class_name': model.__class__.__name__,
        'model_info': model_info,
    }

    save_path = os.path.join(path, f"{pth_name}.pth")
    torch.save(save_dict, save_path)
    return save_path

def load_model_dict(path, pth_name=None):
    """
    save_model_dictë¡œ ì €ì¥í•œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
    ë°˜í™˜ê°’: (model_state, model_info)
    """
    import torch
    load_path = path
    if pth_name is not None:
        load_path = os.path.join(path, f"{pth_name}.pth")
    checkpoint = torch.load(load_path, map_location='cpu', weights_only=False)  # <-- ì—¬ê¸° ì¶”ê°€
    model_state = checkpoint.get('model_state')
    model_info = checkpoint.get('model_info')
    model_info['file_name'] = os.path.basename(load_path)
    return model_state, model_info

################################################################################################################

def search_pth_files(base_path):
    """
    ì…ë ¥ëœ ê²½ë¡œì˜ í•˜ìœ„ í´ë”ë“¤ì—ì„œ pth íŒŒì¼ë“¤ì„ ê²€ìƒ‰
    """
    pth_files = []

    if not os.path.exists(base_path):
        print(f"ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {base_path}")
        return pth_files

    print(f"pth íŒŒì¼ ê²€ìƒ‰ ì‹œì‘: {base_path}")

    # í•˜ìœ„ í´ë”ë“¤ì„ ìˆœíšŒí•˜ë©° pth íŒŒì¼ ê²€ìƒ‰
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.endswith('.pth'):
                pth_path = os.path.join(root, file)
                pth_files.append(pth_path)

    # ê²°ê³¼ ì •ë¦¬ ë° ì¶œë ¥
    if pth_files:
        print(f"\në°œê²¬ëœ pth íŒŒì¼ë“¤ ({len(pth_files)}ê°œ):")
        for i, pth_file in enumerate(pth_files, 1):
            # ìƒëŒ€ ê²½ë¡œë¡œ í‘œì‹œ (base_path ê¸°ì¤€)
            rel_path = os.path.relpath(pth_file, base_path)
            print(f" {i:2d}. {rel_path}")
    else:
        print("pth íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    return pth_files


def save_datasets_as_json(save_datasets, dataset_path):
    """ë°ì´í„°ì…‹ì„ JSON í˜•íƒœë¡œ ì €ì¥"""
    print(f"JSON í˜•íƒœë¡œ ë°ì´í„°ì…‹ ì €ì¥ ì¤‘: {dataset_path}")
    
    # numpy arrayë¥¼ listë¡œ ë³€í™˜
    json_data = {}
    for split in ['train', 'validation', 'test']:
        json_data[split] = {
            'text': save_datasets[split]['text'].tolist() if isinstance(save_datasets[split]['text'], np.ndarray) else list(save_datasets[split]['text']),
            'target': save_datasets[split]['target'].tolist() if isinstance(save_datasets[split]['target'], np.ndarray) else list(save_datasets[split]['target'])
        }
    
    json_data['target_names'] = list(save_datasets['target_names'])
    
    # JSONìœ¼ë¡œ ì €ì¥
    with open(dataset_path, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)
    
    print(f"ì €ì¥ ì™„ë£Œ: {dataset_path}")

def load_datasets_from_json(dataset_path):
    """JSONì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ"""
    print(f"JSONì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ: {dataset_path}")
    
    with open(dataset_path, 'r', encoding='utf-8') as f:
        json_data = json.load(f)
    
    # numpy arrayë¡œ ë³€í™˜
    load_datasets = {}
    for split in ['train', 'validation', 'test']:
        load_datasets[split] = {
            'text': np.array(json_data[split]['text']),
            'target': np.array(json_data[split]['target'])
        }
    
    load_datasets['target_names'] = json_data['target_names']
    
    print("ë¡œë“œ ì™„ë£Œ")
    return load_datasets

################################################################################################################
def create_tqdm(iterable=None, total=None, desc="Progress", **kwargs):
    """
    tqdm ì§„í–‰ë¥  í‘œì‹œì¤„ì„ ìƒì„±í•˜ê±°ë‚˜ ì¬ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜

    Args:
        iterable: ë°˜ë³µ ê°€ëŠ¥í•œ ê°ì²´ (range, list ë“±)
        total: ì „ì²´ ê°œìˆ˜ (iterableì´ Noneì¼ ë•Œ ì‚¬ìš©)
        desc: ì„¤ëª… í…ìŠ¤íŠ¸
        **kwargs: ì¶”ê°€ tqdm ì˜µì…˜

    Returns:
        tqdm ê°ì²´
    """
    # ê¸°ë³¸ ì˜µì…˜ ì„¤ì •
    default_kwargs = get_tqdm_kwargs() if 'get_tqdm_kwargs' in globals() else {}
    default_kwargs.update(kwargs)

    if iterable is not None:
        # iterableì´ ìˆìœ¼ë©´ ì§ì ‘ ì‚¬ìš©
        return tqdm(iterable, desc=desc, **default_kwargs)
    else:
        # manual updateìš© tqdm
        return tqdm(total=total, desc=desc, **default_kwargs)


def reset_tqdm(pbar, iterable=None, total=None, desc=None, **kwargs):
    """
    ê¸°ì¡´ tqdm ê°ì²´ë¥¼ ì¬ì„¤ì •í•˜ëŠ” í•¨ìˆ˜

    Args:
        pbar: ê¸°ì¡´ tqdm ê°ì²´
        iterable: ìƒˆë¡œìš´ ë°˜ë³µ ê°€ëŠ¥í•œ ê°ì²´
        total: ìƒˆë¡œìš´ ì „ì²´ ê°œìˆ˜
        desc: ìƒˆë¡œìš´ ì„¤ëª… í…ìŠ¤íŠ¸
        **kwargs: ì¶”ê°€ ì˜µì…˜

    Returns:
        ì¬ì„¤ì •ëœ tqdm ê°ì²´
    """
    if pbar is None:
        return create_tqdm(iterable, total, desc, **kwargs)

    # ê¸°ì¡´ pbar ì¬ì„¤ì •
    if total is not None:
        pbar.reset(total=total)
    else:
        pbar.reset()

    if desc is not None:
        pbar.set_description(desc)

    # ë‚´ë¶€ ìƒíƒœ ì´ˆê¸°í™”
    pbar.n = 0
    pbar.last_print_n = 0
    pbar.start_t = pbar._time()
    pbar.last_print_t = pbar.start_t

    # ì¶”ê°€ ì˜µì…˜ ì ìš©
    default_kwargs = get_tqdm_kwargs() if 'get_tqdm_kwargs' in globals() else {}
    default_kwargs.update(kwargs)

    for key, value in default_kwargs.items():
        if hasattr(pbar, key):
            setattr(pbar, key, value)

    pbar.refresh()
    return pbar

def create_or_reset_tqdm(pbar=None, iterable=None, total=None, desc="Progress", **kwargs):
    """
    ê¸°ì¡´ create_tqdmê³¼ reset_tqdmì„ í™œìš©í•œ í†µí•© í•¨ìˆ˜

    Args:
        pbar: ê¸°ì¡´ tqdm ê°ì²´ (Noneì´ë©´ ìƒˆë¡œ ìƒì„±)
        iterable: ë°˜ë³µ ê°€ëŠ¥í•œ ê°ì²´
        total: ì „ì²´ ê°œìˆ˜
        desc: ì„¤ëª… í…ìŠ¤íŠ¸
        **kwargs: ì¶”ê°€ tqdm ì˜µì…˜

    Returns:
        tqdm ê°ì²´ (ìƒˆë¡œ ìƒì„±ë˜ê±°ë‚˜ ì¬ì„¤ì •ëœ)
    """
    if pbar is None:
        # ìƒˆë¡œ ìƒì„±
        return create_tqdm(iterable=iterable, total=total, desc=desc, **kwargs)
    else:
        # ê¸°ì¡´ ê²ƒ ì¬ì„¤ì •
        return reset_tqdm(pbar, iterable=iterable, total=total, desc=desc, **kwargs)

##########################################################################################################

def unzip(zipfile_list, remove_zip=False, skip_root=False, normalize_nfc: bool = True, force_utf8: bool = False):
    def _try_force_utf8(name: str) -> str:
        """CP437â†’UTF-8/CP949 ì¬í•´ì„ (ê°œì„ )"""
        candidates = [
            ('utf-8', 'strict'),      # UTF-8 ZIP
            ('cp949', 'ignore'),      # í•œêµ­ ë ˆê±°ì‹œ
            ('euc_kr', 'ignore'),     # êµ¬í˜• ë¦¬ëˆ…ìŠ¤
            ('cp437', 'replace'),     # DOS í´ë°±
        ]
        for enc, errors in candidates:
            try:
                return name.encode('latin1').decode(enc, errors=errors)
            except (UnicodeDecodeError, UnicodeEncodeError):
                continue
        return name  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜

    unzip_paths = []

    for zip_path in zipfile_list:
        if not (os.path.exists(zip_path) and os.path.isfile(zip_path)):
            print(f"ì¡´ì¬í•˜ì§€ ì•Šì€ íŒŒì¼: {zip_path}")
            continue

        extract_dir = zip_path + ".unzip"
        unzip_paths.append(extract_dir)

        if os.path.exists(extract_dir):
            print(f"ì´ë¯¸ ì••ì¶• í•´ì œë¨: {extract_dir}")
            continue

        os.makedirs(extract_dir, exist_ok=True)

        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            members = zip_ref.namelist()

            # ========== skip_root ë¡œì§ (ìˆ˜ì •) ==========
            skip_prefix = ""

            if skip_root and members:
                # ZIP ë‚´ë¶€ ê²½ë¡œëŠ” í•­ìƒ POSIX í˜•ì‹('/')ì´ë¯€ë¡œ '/'ë¡œ ë¶„í• 
                # __MACOSX ê°™ì€ ë©”íƒ€íŒŒì¼ ì œì™¸í•˜ê³  ìµœìƒìœ„ ë””ë ‰í† ë¦¬ ì¶”ì¶œ
                top_level_dirs = set()
                for m in members:
                    if m.startswith('__MACOSX') or m.startswith('.'):
                        continue
                    parts = m.split('/', 1)  # POSIX êµ¬ë¶„ì ì‚¬ìš©
                    if len(parts) > 0 and parts[0]:
                        top_level_dirs.add(parts[0])

                # ìµœìƒìœ„ ë””ë ‰í† ë¦¬ê°€ 1ê°œë§Œ ì¡´ì¬í•˜ë©´ ìŠ¤í‚µ ëŒ€ìƒ
                if len(top_level_dirs) == 1:
                    skip_prefix = list(top_level_dirs)[0] + '/'
                    print(f"ìµœìƒìœ„ ë””ë ‰í† ë¦¬ ìŠ¤í‚µ: {skip_prefix.rstrip('/')}")

            # ========== ì••ì¶• í•´ì œ ==========
            for member_name_orig in tqdm(members, desc=f"ì••ì¶• í•´ì œ ì¤‘: {os.path.basename(zip_path)}", unit="file"):

                # ë©”íƒ€íŒŒì¼ ê±´ë„ˆë›°ê¸°
                if member_name_orig.startswith('__MACOSX') or member_name_orig.startswith('.'):
                    continue

                member_name_to_use = member_name_orig

                # 1. force_utf8 ì˜µì…˜ì´ ì¼œì ¸ìˆìœ¼ë©´ ì¬í•´ì„ ì‹œë„
                if force_utf8:
                    member_name_to_use = _try_force_utf8(member_name_orig)

                # 2. NFD â†’ NFC ë³€í™˜ (ì˜µì…˜)
                if normalize_nfc:
                    member_name_nfc = unicodedata.normalize('NFC', member_name_to_use)
                else:
                    member_name_nfc = member_name_to_use

                # 3. ìµœìƒìœ„ ë””ë ‰í† ë¦¬ ìŠ¤í‚µ (skip_prefixëŠ” ì›ë³¸ ë©¤ë²„ ê¸°ì¤€)
                if skip_prefix and member_name_orig.startswith(skip_prefix):
                    relative_path = member_name_orig[len(skip_prefix):]
                    if not relative_path:
                        continue
                    if force_utf8:
                        relative_path = _try_force_utf8(relative_path)
                    relative_path_nfc = unicodedata.normalize('NFC', relative_path) if normalize_nfc else relative_path
                else:
                    relative_path_nfc = member_name_nfc

                # 4. ì¶”ì¶œ ê²½ë¡œ (OS êµ¬ë¶„ìë¡œ ë³€í™˜)
                target_path = os.path.join(extract_dir, relative_path_nfc.replace('/', os.sep))

                # 5. íŒŒì¼/ë””ë ‰í† ë¦¬ ì¶”ì¶œ
                info = zip_ref.getinfo(member_name_orig)

                if info.is_dir():
                    os.makedirs(target_path, exist_ok=True)
                else:
                    # ìƒìœ„ ë””ë ‰í† ë¦¬ ìƒì„±
                    parent_dir = os.path.dirname(target_path)
                    if parent_dir:
                        os.makedirs(parent_dir, exist_ok=True)

                    # íŒŒì¼ ì¶”ì¶œ
                    with zip_ref.open(member_name_orig) as source, open(target_path, 'wb') as target:
                        target.write(source.read())


            print(f"\nì••ì¶• í•´ì œ ì™„ë£Œ: {extract_dir}")
        # ì›ë³¸ zip ì‚­ì œ
        if remove_zip:
            os.remove(zip_path)
    return unzip_paths


def zip_progress(input_path: Union[str, Path], zip_path: str, compression=None) -> str:
    """
    íŒŒì¼ ë˜ëŠ” í´ë”ë¥¼ ZIPìœ¼ë¡œ ì••ì¶•í•˜ë©´ì„œ tqdm í”„ë¡œê·¸ë ˆìŠ¤ë°”ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
    ì••ì¶• íŒŒì¼ ë‚´ì— ìƒëŒ€ ê²½ë¡œë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.

    Args:
        input_path (Union[str, Path]): ì••ì¶•í•  íŒŒì¼ ë˜ëŠ” í´ë” ê²½ë¡œ.
        zip_path (str): ìƒì„±í•  ZIP íŒŒì¼ ê²½ë¡œ.
        compression (int, optional): ì••ì¶• ë°©ì‹. ê¸°ë³¸ê°’ì€ `zipfile.ZIP_STORED`ì´ë©°, 
                                      `zipfile.ZIP_DEFLATED`ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    Returns:
        str: ìƒì„±ëœ ZIP íŒŒì¼ ê²½ë¡œ. ì‹¤íŒ¨ ì‹œ Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    Raises:
        Exception: ì••ì¶• ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì˜ˆì™¸ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.

    Example:
        >>> zip_progress3("my_folder", "archive.zip")
        Zipping: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 100.00file/s]
        'archive.zip'
    """
    input_path = Path(input_path)
    if not input_path.exists():
        print(f"ì••ì¶•í•  ëŒ€ìƒì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_path}")
        return None

    if compression is None:
        compression = zipfile.ZIP_DEFLATED

    # ì••ì¶• ëŒ€ìƒ íŒŒì¼ ëª©ë¡ ìƒì„±
    if input_path.is_dir():
        files = [f for f in input_path.rglob('*') if f.is_file()]
    else:
        files = [input_path]

    if not files:
        print("ì••ì¶•í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # ZIP íŒŒì¼ ìƒì„±
    with zipfile.ZipFile(zip_path, 'w', compression) as zf:
        with tqdm(total=len(files), desc="Zipping", unit="file") as pbar:
            for file in files:
                # ì••ì¶• íŒŒì¼ ë‚´ ìƒëŒ€ ê²½ë¡œ ê³„ì‚°
                arcname = file.relative_to(input_path.parent if input_path.is_file() else input_path)
                zf.write(file, arcname)
                pbar.update(1)

    return zip_path


##########################################################################################################

print('helper_utils.py loaded')
